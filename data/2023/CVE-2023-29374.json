{
	"id": "CVE-2023-29374",
	"vulnerabilities": [
		{
			"content": {
				"id": "CVE-2023-29374",
				"description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
				"references": [
					{
						"source": "CVE",
						"url": "https://github.com/hwchase17/langchain/issues/1026"
					},
					{
						"source": "CVE",
						"url": "https://github.com/hwchase17/langchain/issues/814"
					},
					{
						"source": "CVE",
						"url": "https://github.com/hwchase17/langchain/pull/1119"
					},
					{
						"source": "CVE",
						"url": "https://twitter.com/rharang/status/1641899743608463365/photo/1"
					},
					{
						"source": "mitre",
						"url": "https://github.com/hwchase17/langchain/issues/1026"
					},
					{
						"source": "mitre",
						"url": "https://github.com/hwchase17/langchain/issues/814"
					},
					{
						"source": "mitre",
						"url": "https://github.com/hwchase17/langchain/pull/1119"
					},
					{
						"source": "mitre",
						"url": "https://twitter.com/rharang/status/1641899743608463365/photo/1"
					}
				],
				"modified": "2024-08-02T14:07:45.736Z"
			}
		}
	],
	"data_source": "mitre-v5"
}
